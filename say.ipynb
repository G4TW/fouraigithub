#@title
Text = input("What would you like to say?") #@param {type:"string"}
Rawopt = input("Rawopt? True or False") #@param {type:"boolean"}
text = Text
 
sigma = 0.75
denoise_strength = 0.01
raw_input_ = Rawopt  # disables automatic ARPAbet conversion, useful for inputting your own pronounciation or just for testing 
 
show_graphs = True
graph_scale = 1 # literally a zoom factor
alignment_graph_width = 1800
alignment_graph_height = 720
 
save_wavs = 1
counter = 0
text = unidecode(text) # convert unicode punctuation into it's normal equivalents (thanks Fimfiction.)
text = text * 1 # how many times to generate each clip
with torch.no_grad():
    for i in text.split("\n"):
        if len(i) < 1: continue;
        print(i)
        if raw_input_:
            if i[-1] != "␤": i=i+"␤" 
        else: i = ARPA(i)
        print(i)
        sequence = np.array(text_to_sequence(i, ['english_cleaners']))[None, :]
        sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long()
        mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)
        if show_graphs:
            plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],
                alignments.float().data.cpu().numpy()[0].T))
        audio = waveglow.infer(mel_outputs_postnet, sigma=sigma); print(""); ipd.display(ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate))
        audio_denoised = denoiser(audio, strength=denoise_strength)[:, 0]; print("Denoised"); ipd.display(ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate))
        if save_wavs:
            librosa.output.write_wav('tacotron2/infer/Inf_' + str(counter) + '.wav', np.swapaxes(audio_denoised.cpu().numpy(),0,1), hparams.sampling_rate)
        counter+=1
